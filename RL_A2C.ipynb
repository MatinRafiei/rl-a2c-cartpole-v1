{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Dependencies** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# import gymnasium library of OpenAI to create a environment\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Policy Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        # define first fully connected layer --- the input size is same as our state_size and the output size is same as the hidden_size (**SET HYPERPARAMETERS**)\n",
    "        self.FullyConnectedLayer_1 = nn.Linear(state_size, hidden_size)\n",
    "        \n",
    "        # define first fully connected layer --- the input size is same as our hidden_size as the output of the last layer and the output size is same as action_size\n",
    "        self.FullyConnectedLayer_2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    # define forward function\n",
    "        \"\"\"\n",
    "         1. state go to the first layer as input\n",
    "          2. relu activation function apply on the output of first layer\n",
    "           3. the output of relu function goes into the second layer as input\n",
    "            4. softmax activation function apply on the output of the second layer\n",
    "             5. the output of the second layer go to the Categorical Function as input that gives us a probability distribution over actions\n",
    "              6. sampling one action in the distribution and return\n",
    "              \"\"\"\n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        state = nn.functional.relu(self.FullyConnectedLayer_1(state))\n",
    "        state = self.FullyConnectedLayer_2(state)\n",
    "        state =  nn.functional.softmax(state, dim=1)\n",
    "        policy_dist = Categorical(state)\n",
    "        action = policy_dist.sample()\n",
    "        return state, action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Value Function Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size): # note that the output of this network is a value of a action (the action that done by policy network), so the output size is equal to 1.\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # define first fully connected layer --- the input size is same as our state_size and the output size is same as the hidden_size (**SET HYPERPARAMETERS**)\n",
    "        self.FullyConnectedLayer_1 = nn.Linear(state_size, hidden_size)\n",
    "        \n",
    "        # define first fully connected layer --- the input size is same as our hidden_size as the output of the last layer and the output size is same as action_size\n",
    "        self.FullyConnectedLayer_2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    # define forward function\n",
    "        \"\"\"\n",
    "         1. state go to the first layer as input\n",
    "          2. relu activation function apply on the output of first layer\n",
    "           3. the output of relu function goes into the second layer as input and return\n",
    "              \"\"\"\n",
    "    def forward(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        state = nn.functional.relu(self.FullyConnectedLayer_1(state))\n",
    "        state = self.FullyConnectedLayer_2(state)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Advantage Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the figure below show the definition of advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-media-1.freecodecamp.org/images/1*SvSFYWx5-u5zf38baqBgyQ.png\"  width=\"25%\" height=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we estimate Q(s,a) as TD error, so the formula convert to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-media-1.freecodecamp.org/images/1*fmWayfCY4QVIounYXWi2rg.png\"  width=\"25%\" height=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so for define the advantage function, we need reward of an action, value of the next state, discount factor and value of the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvantageFunction(reward, next_value, current_value, discount_factor):\n",
    "    advantage = reward + discount_factor * next_value.detach() - current_value.detach()   # note that detach() returns a new tensor that doesn't require a gradient (dont follow gradient computation)\n",
    "    # print(f\"next value: {next_value}\")\n",
    "    return advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Policy Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/lxdvqu1lno5xulbujb9l.jpeg\"  width=\"25%\" height=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the figure show that the poliucy loss is equal to logarithm of probability of a action * advantage of that action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyLoss(prob_dist, action, advantage):\n",
    "\n",
    "    # note that the negative sign is used to convert the problem of maximizing the expected return into a problem of minimizing the negative expected return.(gradient descent minimize the error)\n",
    "\n",
    "    \"\"\" \n",
    "     to stabilize the training process and prevent the policy from changing too much in a single update, we define regularization term as\n",
    "      0.01 * advantage.pow(2) that is the squared L2 norm of the advantage, which is a measure of how much variance there is in the advantage estimates.\n",
    "       By adding this term to the policy loss, we encourage the policy to be more stable and smooth. \"\"\"\n",
    "    # prob_dist.item()[action]\n",
    "    policy_loss = -(torch.log(prob_dist[0, action]).detach() * advantage.detach() + 0.01 * advantage.pow(2))\n",
    "    policy_loss = torch.tensor(policy_loss, requires_grad=True)\n",
    "    return policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Value Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value loss is a measure of how well the value network is doing at estimating the expected return.\\\n",
    "It is computed as **the mean squared error between the actual return and the estimated return**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueLoss(advantage):\n",
    "    value_loss = 0.5 * advantage.pow(2)\n",
    "    value_loss = torch.tensor(value_loss, requires_grad=True)\n",
    "    return value_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Environment** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.0001\n",
    "discount_factor = 0.99\n",
    "num_epochs = 1000 # 1000\n",
    "num_steps = 200 # 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Policy and Value Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Policy Network\n",
    "Actor_Net = Actor(state_size, action_size, hidden_size)\n",
    "\n",
    "# Create Value Function Network\n",
    "Critic_Net = Critic(state_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define an Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_list = list(Actor_Net.parameters()) + list(Critic_Net.parameters())\n",
    "optimizer = optim.Adam(parameters_list, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Train Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a blank list to save total reward of each epoch\n",
    "epochs_rewards = []\n",
    "\n",
    "# define a blank list to save total loss of each epoch\n",
    "epochs_loss = []\n",
    "\n",
    "# define a blank list to save the length of each epoch\n",
    "epochs_length = []\n",
    "def train(env, Actor_Net, Critic_Net, optimizer, num_epochs, num_steps):\n",
    "\n",
    "    for episode in range(num_epochs):\n",
    "\n",
    "        # this variable show that the episode has been ended or no, if it has been ended, done is True and no action can be taken\n",
    "        # we set this variable to False at the beginnig of each episode\n",
    "        done = False\n",
    "\n",
    "        # reset the environment in the beginning of each episode\n",
    "        state = env.reset()[0]\n",
    "\n",
    "        # define a blank list to save the reward of each step in a specific epoch\n",
    "        rewards = []\n",
    "\n",
    "        # define a blank list to save the loss of each step in a specific epoch\n",
    "        loss = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            \n",
    "            \n",
    "            # applying state to each network and give action & action_value\n",
    "            prob_dist , action = Actor_Net(state)\n",
    "            # print(policy_dist.detach().numpy()[0])\n",
    "            action_value = Critic_Net(state)\n",
    "            # applying action to the environment and give (next_state, reward, done , _ , _)\n",
    "            next_state, reward, done, _ , _ = env.step(action.item())\n",
    "\n",
    "            # save the reward that come from env to the reward list\n",
    "            rewards.append(reward)\n",
    "\n",
    "            \n",
    "            next_value = Critic_Net(next_state)\n",
    "            advantage = AdvantageFunction(reward, next_value, action_value, discount_factor)\n",
    "            policy_loss = PolicyLoss(prob_dist, action, advantage)\n",
    "            value_loss = ValueLoss(advantage)\n",
    "            total_loss = policy_loss + value_loss\n",
    "            loss.append(total_loss)\n",
    "            # total_loss.backward()\n",
    "            policy_loss.backward()\n",
    "            value_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            state = next_state\n",
    "\n",
    "            # print(f\"episode: {episode + 1}, step: {step + 1}, action: {action.item()}, reward: {reward}, Policy loss: {policy_loss.item()}, Value loss: {value_loss.item()}\")\n",
    "\n",
    "            if done:\n",
    "                epochs_length.append(step + 1)\n",
    "                break\n",
    "\n",
    "        epochs_rewards.append(sum(rewards))\n",
    "        epochs_loss.append(sum(loss))\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"episode: {episode}, reward: {epochs_rewards[episode]}, total length: {epochs_length[episode]}, total loss: {epochs_loss[episode]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M.Rodriguez\\AppData\\Local\\Temp\\ipykernel_7648\\3252505967.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  policy_loss = torch.tensor(policy_loss, requires_grad=True)\n",
      "C:\\Users\\M.Rodriguez\\AppData\\Local\\Temp\\ipykernel_7648\\2729072174.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value_loss = torch.tensor(value_loss, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 9.0, total length: 9, total loss: tensor([[11.0779]], grad_fn=<AddBackward0>)\n",
      "episode: 10, reward: 25.0, total length: 25, total loss: tensor([[29.4721]], grad_fn=<AddBackward0>)\n",
      "episode: 20, reward: 11.0, total length: 11, total loss: tensor([[13.2655]], grad_fn=<AddBackward0>)\n",
      "episode: 30, reward: 27.0, total length: 27, total loss: tensor([[31.7683]], grad_fn=<AddBackward0>)\n",
      "episode: 40, reward: 17.0, total length: 17, total loss: tensor([[20.1618]], grad_fn=<AddBackward0>)\n",
      "episode: 50, reward: 26.0, total length: 26, total loss: tensor([[31.3910]], grad_fn=<AddBackward0>)\n",
      "episode: 60, reward: 17.0, total length: 17, total loss: tensor([[20.3263]], grad_fn=<AddBackward0>)\n",
      "episode: 70, reward: 31.0, total length: 31, total loss: tensor([[36.7293]], grad_fn=<AddBackward0>)\n",
      "episode: 80, reward: 23.0, total length: 23, total loss: tensor([[27.3833]], grad_fn=<AddBackward0>)\n",
      "episode: 90, reward: 12.0, total length: 12, total loss: tensor([[14.2215]], grad_fn=<AddBackward0>)\n",
      "episode: 100, reward: 14.0, total length: 14, total loss: tensor([[16.6256]], grad_fn=<AddBackward0>)\n",
      "episode: 110, reward: 19.0, total length: 19, total loss: tensor([[22.4760]], grad_fn=<AddBackward0>)\n",
      "episode: 120, reward: 20.0, total length: 20, total loss: tensor([[23.7470]], grad_fn=<AddBackward0>)\n",
      "episode: 130, reward: 14.0, total length: 14, total loss: tensor([[16.8604]], grad_fn=<AddBackward0>)\n",
      "episode: 140, reward: 23.0, total length: 23, total loss: tensor([[27.0282]], grad_fn=<AddBackward0>)\n",
      "episode: 150, reward: 12.0, total length: 12, total loss: tensor([[14.4784]], grad_fn=<AddBackward0>)\n",
      "episode: 160, reward: 37.0, total length: 37, total loss: tensor([[43.4583]], grad_fn=<AddBackward0>)\n",
      "episode: 170, reward: 29.0, total length: 29, total loss: tensor([[34.2786]], grad_fn=<AddBackward0>)\n",
      "episode: 180, reward: 13.0, total length: 13, total loss: tensor([[16.1227]], grad_fn=<AddBackward0>)\n",
      "episode: 190, reward: 13.0, total length: 13, total loss: tensor([[15.4319]], grad_fn=<AddBackward0>)\n",
      "episode: 200, reward: 27.0, total length: 27, total loss: tensor([[32.1601]], grad_fn=<AddBackward0>)\n",
      "episode: 210, reward: 15.0, total length: 15, total loss: tensor([[17.9648]], grad_fn=<AddBackward0>)\n",
      "episode: 220, reward: 43.0, total length: 43, total loss: tensor([[50.8024]], grad_fn=<AddBackward0>)\n",
      "episode: 230, reward: 30.0, total length: 30, total loss: tensor([[35.3868]], grad_fn=<AddBackward0>)\n",
      "episode: 240, reward: 76.0, total length: 76, total loss: tensor([[89.4329]], grad_fn=<AddBackward0>)\n",
      "episode: 250, reward: 23.0, total length: 23, total loss: tensor([[27.3845]], grad_fn=<AddBackward0>)\n",
      "episode: 260, reward: 31.0, total length: 31, total loss: tensor([[36.4263]], grad_fn=<AddBackward0>)\n",
      "episode: 270, reward: 25.0, total length: 25, total loss: tensor([[29.3752]], grad_fn=<AddBackward0>)\n",
      "episode: 280, reward: 14.0, total length: 14, total loss: tensor([[16.6254]], grad_fn=<AddBackward0>)\n",
      "episode: 290, reward: 19.0, total length: 19, total loss: tensor([[22.3329]], grad_fn=<AddBackward0>)\n",
      "episode: 300, reward: 26.0, total length: 26, total loss: tensor([[30.7103]], grad_fn=<AddBackward0>)\n",
      "episode: 310, reward: 13.0, total length: 13, total loss: tensor([[15.4456]], grad_fn=<AddBackward0>)\n",
      "episode: 320, reward: 23.0, total length: 23, total loss: tensor([[27.1679]], grad_fn=<AddBackward0>)\n",
      "episode: 330, reward: 49.0, total length: 49, total loss: tensor([[57.8500]], grad_fn=<AddBackward0>)\n",
      "episode: 340, reward: 28.0, total length: 28, total loss: tensor([[33.1255]], grad_fn=<AddBackward0>)\n",
      "episode: 350, reward: 31.0, total length: 31, total loss: tensor([[36.3648]], grad_fn=<AddBackward0>)\n",
      "episode: 360, reward: 11.0, total length: 11, total loss: tensor([[13.1124]], grad_fn=<AddBackward0>)\n",
      "episode: 370, reward: 19.0, total length: 19, total loss: tensor([[22.4888]], grad_fn=<AddBackward0>)\n",
      "episode: 380, reward: 34.0, total length: 34, total loss: tensor([[40.5671]], grad_fn=<AddBackward0>)\n",
      "episode: 390, reward: 21.0, total length: 21, total loss: tensor([[24.6375]], grad_fn=<AddBackward0>)\n",
      "episode: 400, reward: 19.0, total length: 19, total loss: tensor([[22.4858]], grad_fn=<AddBackward0>)\n",
      "episode: 410, reward: 18.0, total length: 18, total loss: tensor([[21.5886]], grad_fn=<AddBackward0>)\n",
      "episode: 420, reward: 19.0, total length: 19, total loss: tensor([[22.9345]], grad_fn=<AddBackward0>)\n",
      "episode: 430, reward: 12.0, total length: 12, total loss: tensor([[14.3246]], grad_fn=<AddBackward0>)\n",
      "episode: 440, reward: 15.0, total length: 15, total loss: tensor([[17.7610]], grad_fn=<AddBackward0>)\n",
      "episode: 450, reward: 26.0, total length: 26, total loss: tensor([[30.6612]], grad_fn=<AddBackward0>)\n",
      "episode: 460, reward: 27.0, total length: 27, total loss: tensor([[31.7079]], grad_fn=<AddBackward0>)\n",
      "episode: 470, reward: 16.0, total length: 16, total loss: tensor([[19.0478]], grad_fn=<AddBackward0>)\n",
      "episode: 480, reward: 11.0, total length: 11, total loss: tensor([[13.1763]], grad_fn=<AddBackward0>)\n",
      "episode: 490, reward: 15.0, total length: 15, total loss: tensor([[17.8152]], grad_fn=<AddBackward0>)\n",
      "episode: 500, reward: 10.0, total length: 10, total loss: tensor([[11.9789]], grad_fn=<AddBackward0>)\n",
      "episode: 510, reward: 32.0, total length: 32, total loss: tensor([[37.7788]], grad_fn=<AddBackward0>)\n",
      "episode: 520, reward: 40.0, total length: 40, total loss: tensor([[47.4951]], grad_fn=<AddBackward0>)\n",
      "episode: 530, reward: 18.0, total length: 18, total loss: tensor([[21.3066]], grad_fn=<AddBackward0>)\n",
      "episode: 540, reward: 14.0, total length: 14, total loss: tensor([[17.1441]], grad_fn=<AddBackward0>)\n",
      "episode: 550, reward: 36.0, total length: 36, total loss: tensor([[42.3583]], grad_fn=<AddBackward0>)\n",
      "episode: 560, reward: 13.0, total length: 13, total loss: tensor([[15.5824]], grad_fn=<AddBackward0>)\n",
      "episode: 570, reward: 24.0, total length: 24, total loss: tensor([[28.4538]], grad_fn=<AddBackward0>)\n",
      "episode: 580, reward: 18.0, total length: 18, total loss: tensor([[21.5845]], grad_fn=<AddBackward0>)\n",
      "episode: 590, reward: 17.0, total length: 17, total loss: tensor([[20.0011]], grad_fn=<AddBackward0>)\n",
      "episode: 600, reward: 38.0, total length: 38, total loss: tensor([[44.6193]], grad_fn=<AddBackward0>)\n",
      "episode: 610, reward: 28.0, total length: 28, total loss: tensor([[33.1196]], grad_fn=<AddBackward0>)\n",
      "episode: 620, reward: 37.0, total length: 37, total loss: tensor([[43.5186]], grad_fn=<AddBackward0>)\n",
      "episode: 630, reward: 22.0, total length: 22, total loss: tensor([[25.8911]], grad_fn=<AddBackward0>)\n",
      "episode: 640, reward: 13.0, total length: 13, total loss: tensor([[15.2665]], grad_fn=<AddBackward0>)\n",
      "episode: 650, reward: 20.0, total length: 20, total loss: tensor([[23.8226]], grad_fn=<AddBackward0>)\n",
      "episode: 660, reward: 18.0, total length: 18, total loss: tensor([[21.4987]], grad_fn=<AddBackward0>)\n",
      "episode: 670, reward: 16.0, total length: 16, total loss: tensor([[18.9856]], grad_fn=<AddBackward0>)\n",
      "episode: 680, reward: 13.0, total length: 13, total loss: tensor([[15.3833]], grad_fn=<AddBackward0>)\n",
      "episode: 690, reward: 18.0, total length: 18, total loss: tensor([[21.5501]], grad_fn=<AddBackward0>)\n",
      "episode: 700, reward: 25.0, total length: 25, total loss: tensor([[29.4823]], grad_fn=<AddBackward0>)\n",
      "episode: 710, reward: 43.0, total length: 43, total loss: tensor([[50.6660]], grad_fn=<AddBackward0>)\n",
      "episode: 720, reward: 26.0, total length: 26, total loss: tensor([[30.8510]], grad_fn=<AddBackward0>)\n",
      "episode: 730, reward: 18.0, total length: 18, total loss: tensor([[21.4213]], grad_fn=<AddBackward0>)\n",
      "episode: 740, reward: 19.0, total length: 19, total loss: tensor([[22.3084]], grad_fn=<AddBackward0>)\n",
      "episode: 750, reward: 22.0, total length: 22, total loss: tensor([[26.0902]], grad_fn=<AddBackward0>)\n",
      "episode: 760, reward: 18.0, total length: 18, total loss: tensor([[21.7380]], grad_fn=<AddBackward0>)\n",
      "episode: 770, reward: 23.0, total length: 23, total loss: tensor([[27.5526]], grad_fn=<AddBackward0>)\n",
      "episode: 780, reward: 19.0, total length: 19, total loss: tensor([[22.5305]], grad_fn=<AddBackward0>)\n",
      "episode: 790, reward: 25.0, total length: 25, total loss: tensor([[29.5803]], grad_fn=<AddBackward0>)\n",
      "episode: 800, reward: 15.0, total length: 15, total loss: tensor([[17.9126]], grad_fn=<AddBackward0>)\n",
      "episode: 810, reward: 14.0, total length: 14, total loss: tensor([[16.5845]], grad_fn=<AddBackward0>)\n",
      "episode: 820, reward: 11.0, total length: 11, total loss: tensor([[13.4513]], grad_fn=<AddBackward0>)\n",
      "episode: 830, reward: 25.0, total length: 25, total loss: tensor([[29.9221]], grad_fn=<AddBackward0>)\n",
      "episode: 840, reward: 49.0, total length: 49, total loss: tensor([[58.0567]], grad_fn=<AddBackward0>)\n",
      "episode: 850, reward: 55.0, total length: 55, total loss: tensor([[64.6551]], grad_fn=<AddBackward0>)\n",
      "episode: 860, reward: 9.0, total length: 9, total loss: tensor([[10.7743]], grad_fn=<AddBackward0>)\n",
      "episode: 870, reward: 12.0, total length: 12, total loss: tensor([[14.4852]], grad_fn=<AddBackward0>)\n",
      "episode: 880, reward: 18.0, total length: 18, total loss: tensor([[21.4913]], grad_fn=<AddBackward0>)\n",
      "episode: 890, reward: 44.0, total length: 44, total loss: tensor([[51.9508]], grad_fn=<AddBackward0>)\n",
      "episode: 900, reward: 41.0, total length: 41, total loss: tensor([[48.2926]], grad_fn=<AddBackward0>)\n",
      "episode: 910, reward: 18.0, total length: 18, total loss: tensor([[21.3939]], grad_fn=<AddBackward0>)\n",
      "episode: 920, reward: 36.0, total length: 36, total loss: tensor([[42.4568]], grad_fn=<AddBackward0>)\n",
      "episode: 930, reward: 14.0, total length: 14, total loss: tensor([[16.6508]], grad_fn=<AddBackward0>)\n",
      "episode: 940, reward: 27.0, total length: 27, total loss: tensor([[31.8296]], grad_fn=<AddBackward0>)\n",
      "episode: 950, reward: 19.0, total length: 19, total loss: tensor([[22.5740]], grad_fn=<AddBackward0>)\n",
      "episode: 960, reward: 32.0, total length: 32, total loss: tensor([[37.7423]], grad_fn=<AddBackward0>)\n",
      "episode: 970, reward: 26.0, total length: 26, total loss: tensor([[31.0925]], grad_fn=<AddBackward0>)\n",
      "episode: 980, reward: 29.0, total length: 29, total loss: tensor([[34.2186]], grad_fn=<AddBackward0>)\n",
      "episode: 990, reward: 19.0, total length: 19, total loss: tensor([[22.9149]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train(env, Actor_Net, Critic_Net, optimizer, num_epochs, num_steps):\n",
    "train(env, Actor_Net, Critic_Net, optimizer, num_epochs, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
